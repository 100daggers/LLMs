{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c3f79d-5bf2-4af3-8b8d-c5fa6d9081d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 87599\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"qa_dataset/squad_train.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(\"Number of entries:\", len(data))\n",
    "data_mapped = []\n",
    "for i in range(len(data)):\n",
    "    data_mapped.append({\n",
    "        \"instruction\" : f\"Answer the following question based on the context: {data[i]['context']}\",\n",
    "        \"input\" : data[i][\"question\"],\n",
    "        \"output\" : data[i][\"answer\"]}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49acebe0-a480-4351-ba32-555eb69e8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_mapped\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.15)    # 15% for testing\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4b3c0c-1a2f-4e5e-ac55-870ef4ac0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 74459\n",
      "Test set length: 13140\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Test set length:\", len(test_data))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e28d5d7d-6f0c-43af-82a6-12a641543520",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\", \"w\") as json_file:\n",
    "    json.dump(train_data, json_file, indent=4)\n",
    "    \n",
    "with open(\"test.json\", \"w\") as json_file:\n",
    "    json.dump(test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724d0550-8cbb-45e4-9902-4e993eb7dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop. Please install uvloop for better performance using `pip install uvloop`.\n",
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "usage: litgpt [options] finetune_lora [-h] [--config CONFIG]\n",
      "                                      [--print_config[=flags]]\n",
      "                                      [--out_dir OUT_DIR]\n",
      "                                      [--precision PRECISION]\n",
      "                                      [--quantize QUANTIZE]\n",
      "                                      [--devices DEVICES]\n",
      "                                      [--num_nodes NUM_NODES]\n",
      "                                      [--lora_r LORA_R]\n",
      "                                      [--lora_alpha LORA_ALPHA]\n",
      "                                      [--lora_dropout LORA_DROPOUT]\n",
      "                                      [--lora_query {true,false}]\n",
      "                                      [--lora_key {true,false}]\n",
      "                                      [--lora_value {true,false}]\n",
      "                                      [--lora_projection {true,false}]\n",
      "                                      [--lora_mlp {true,false}]\n",
      "                                      [--lora_head {true,false}]\n",
      "                                      [--data.help CLASS_PATH_OR_NAME]\n",
      "                                      [--data DATA] [--train CONFIG]\n",
      "                                      [--train.save_interval SAVE_INTERVAL]\n",
      "                                      [--train.log_interval LOG_INTERVAL]\n",
      "                                      [--train.global_batch_size GLOBAL_BATCH_SIZE]\n",
      "                                      [--train.micro_batch_size MICRO_BATCH_SIZE]\n",
      "                                      [--train.lr_warmup_steps LR_WARMUP_STEPS]\n",
      "                                      [--train.lr_warmup_fraction LR_WARMUP_FRACTION]\n",
      "                                      [--train.epochs EPOCHS]\n",
      "                                      [--train.max_tokens MAX_TOKENS]\n",
      "                                      [--train.max_steps MAX_STEPS]\n",
      "                                      [--train.max_seq_length MAX_SEQ_LENGTH]\n",
      "                                      [--train.tie_embeddings {true,false,null}]\n",
      "                                      [--train.max_norm MAX_NORM]\n",
      "                                      [--train.min_lr MIN_LR] [--eval CONFIG]\n",
      "                                      [--eval.interval INTERVAL]\n",
      "                                      [--eval.max_new_tokens MAX_NEW_TOKENS]\n",
      "                                      [--eval.max_iters MAX_ITERS]\n",
      "                                      [--eval.initial_validation {true,false}]\n",
      "                                      [--eval.final_validation {true,false}]\n",
      "                                      [--eval.evaluate_example EVALUATE_EXAMPLE]\n",
      "                                      [--optimizer OPTIMIZER]\n",
      "                                      [--logger_name {wandb,tensorboard,csv}]\n",
      "                                      [--seed SEED]\n",
      "                                      [--access_token ACCESS_TOKEN]\n",
      "                                      checkpoint_dir\n",
      "\n",
      "Finetune a model using the LoRA method.\n",
      "\n",
      "positional arguments:\n",
      "  checkpoint_dir        The path to the base model's checkpoint directory to\n",
      "                        load for finetuning. (required, type: <class 'Path'>)\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  --config CONFIG       Path to a configuration file.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "  --out_dir OUT_DIR     Directory in which to save checkpoints and logs. If\n",
      "                        running in a Lightning Studio Job, look for it in\n",
      "                        /teamspace/jobs/<job-name>/share. (type: <class\n",
      "                        'Path'>, default: out\\finetune\\lora)\n",
      "  --precision PRECISION\n",
      "                        The precision to use for finetuning. Possible choices:\n",
      "                        \"bf16-true\", \"bf16-mixed\", \"32-true\". (type:\n",
      "                        Optional[str], default: null)\n",
      "  --quantize QUANTIZE   If set, quantize the model with this algorithm. See\n",
      "                        ``tutorials/quantize.md`` for more information. (type:\n",
      "                        Optional[Literal['nf4', 'nf4-dq', 'fp4', 'fp4-dq',\n",
      "                        'int8-training']], default: null)\n",
      "  --devices DEVICES     How many devices/GPUs to use. (type: Union[int, str],\n",
      "                        default: 1)\n",
      "  --num_nodes NUM_NODES\n",
      "                        How many nodes the code is being run on. (type: int,\n",
      "                        default: 1)\n",
      "  --lora_r LORA_R       The LoRA rank. (type: int, default: 8)\n",
      "  --lora_alpha LORA_ALPHA\n",
      "                        The LoRA alpha. (type: int, default: 16)\n",
      "  --lora_dropout LORA_DROPOUT\n",
      "                        The LoRA dropout value. (type: float, default: 0.05)\n",
      "  --lora_query {true,false}\n",
      "                        Whether to apply LoRA to the query weights in\n",
      "                        attention. (type: bool, default: True)\n",
      "  --lora_key {true,false}\n",
      "                        Whether to apply LoRA to the key weights in attention.\n",
      "                        (type: bool, default: False)\n",
      "  --lora_value {true,false}\n",
      "                        Whether to apply LoRA to the value weights in\n",
      "                        attention. (type: bool, default: True)\n",
      "  --lora_projection {true,false}\n",
      "                        Whether to apply LoRA to the output projection in the\n",
      "                        attention block. (type: bool, default: False)\n",
      "  --lora_mlp {true,false}\n",
      "                        Whether to apply LoRA to the weights of the MLP in the\n",
      "                        attention block. (type: bool, default: False)\n",
      "  --lora_head {true,false}\n",
      "                        Whether to apply LoRA to output head in GPT. (type:\n",
      "                        bool, default: False)\n",
      "  --data.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of DataModule and\n",
      "                        exit.\n",
      "  --data DATA           Data-related arguments. If not provided, the default\n",
      "                        is ``litgpt.data.Alpaca``. (type:\n",
      "                        Optional[DataModule], default: null, known subclasses:\n",
      "                        litgpt.data.DataModule, litgpt.data.Alpaca,\n",
      "                        litgpt.data.Alpaca2k, litgpt.data.AlpacaGPT4,\n",
      "                        litgpt.data.JSON, litgpt.data.Deita, litgpt.data.FLAN,\n",
      "                        litgpt.data.LIMA, litgpt.data.LitData,\n",
      "                        litgpt.data.LongForm, litgpt.data.TextFiles,\n",
      "                        litgpt.data.TinyLlama, litgpt.data.MicroLlama,\n",
      "                        litgpt.data.TinyStories, litgpt.data.OpenWebText)\n",
      "  --optimizer OPTIMIZER\n",
      "                        An optimizer name (such as \"AdamW\") or config. (type:\n",
      "                        Union[str, Dict], default: AdamW)\n",
      "  --logger_name {wandb,tensorboard,csv}\n",
      "                        The name of the logger to send metrics to. (type:\n",
      "                        Literal['wandb', 'tensorboard', 'csv'], default: csv)\n",
      "  --seed SEED           The random seed to use for reproducibility. (type:\n",
      "                        int, default: 1337)\n",
      "  --access_token ACCESS_TOKEN\n",
      "                        Optional API token to access models with restrictions.\n",
      "                        (type: Optional[str], default: null)\n",
      "\n",
      "Training-related arguments. See ``litgpt.args.TrainArgs`` for details:\n",
      "  --train CONFIG        Path to a configuration file.\n",
      "  --train.save_interval SAVE_INTERVAL\n",
      "                        Number of optimizer steps between saving checkpoints\n",
      "                        (type: Optional[int], default: 1000)\n",
      "  --train.log_interval LOG_INTERVAL\n",
      "                        Number of iterations between logging calls (type: int,\n",
      "                        default: 1)\n",
      "  --train.global_batch_size GLOBAL_BATCH_SIZE\n",
      "                        Number of samples between optimizer steps across data-\n",
      "                        parallel ranks (type: int, default: 16)\n",
      "  --train.micro_batch_size MICRO_BATCH_SIZE\n",
      "                        Number of samples per data-parallel rank (type: int,\n",
      "                        default: 1)\n",
      "  --train.lr_warmup_steps LR_WARMUP_STEPS\n",
      "                        Number of iterations with learning rate warmup active\n",
      "                        (type: Optional[int], default: 100)\n",
      "  --train.lr_warmup_fraction LR_WARMUP_FRACTION\n",
      "                        The fraction of an epoch to use for learning rate\n",
      "                        warmup (type: Optional[float], default: null)\n",
      "  --train.epochs EPOCHS\n",
      "                        Number of epochs to train on (type: Optional[int],\n",
      "                        default: 5)\n",
      "  --train.max_tokens MAX_TOKENS\n",
      "                        Total number of tokens to train on (type:\n",
      "                        Optional[int], default: null)\n",
      "  --train.max_steps MAX_STEPS\n",
      "                        Limits the number of optimizer steps to run (type:\n",
      "                        Optional[int], default: null)\n",
      "  --train.max_seq_length MAX_SEQ_LENGTH\n",
      "                        Limits the length of samples (type: Optional[int],\n",
      "                        default: null)\n",
      "  --train.tie_embeddings {true,false,null}\n",
      "                        Whether to tie the embedding weights with the language\n",
      "                        modeling head weights (type: Optional[bool], default:\n",
      "                        null)\n",
      "  --train.max_norm MAX_NORM\n",
      "                        (type: Optional[float], default: null)\n",
      "  --train.min_lr MIN_LR\n",
      "                        (type: float, default: 6e-05)\n",
      "\n",
      "Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details:\n",
      "  --eval CONFIG         Path to a configuration file.\n",
      "  --eval.interval INTERVAL\n",
      "                        Number of optimizer steps between evaluation calls\n",
      "                        (type: int, default: 100)\n",
      "  --eval.max_new_tokens MAX_NEW_TOKENS\n",
      "                        Number of tokens to generate (type: Optional[int],\n",
      "                        default: 100)\n",
      "  --eval.max_iters MAX_ITERS\n",
      "                        Number of iterations (type: int, default: 100)\n",
      "  --eval.initial_validation {true,false}\n",
      "                        Whether to evaluate on the validation set at the\n",
      "                        beginning of the training (type: bool, default: False)\n",
      "  --eval.final_validation {true,false}\n",
      "                        Whether to evaluate on the validation set at the end\n",
      "                        of the training (type: bool, default: True)\n",
      "  --eval.evaluate_example EVALUATE_EXAMPLE\n",
      "                        How to pick an example instruction to evaluate\n",
      "                        periodically during training. Can be \"first\",\n",
      "                        \"random\", or an integer index to pick a specific\n",
      "                        example. (type: Union[str, int], default: first)\n"
     ]
    }
   ],
   "source": [
    "!litgpt finetune_lora --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b2eba9-c8d0-44fa-9922-6749e27dd120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a014fa9-e3d4-4d8a-a8d5-73d7425f4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in command prompt\n",
    "#!litgpt finetune_lora meta-llama\\Llama-3.2-1B-Instruct --data JSON data.val_split_fraction 0.1 --data.json_path train.json --train.epochs 3 --train.log_interval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c85c31-666f-4eef-abd3-69df39bc26a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
